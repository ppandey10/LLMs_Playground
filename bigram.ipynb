{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start by looking at the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270153\n"
     ]
    }
   ],
   "source": [
    "with open('The_Great_Gatsby.txt', 'r', encoding='utf-8') as file:\n",
    "    txt_file = file.read()\n",
    "print(len(txt_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to construct a vocabulary and for that I will need characters in my file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ç', 'é', 'ê', 'ô', '\\u200a', '—', '‘', '’', '“', '”', '…', '\\ufeff']\n",
      "Total number of unique characters/Vocabulary size: 88\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(txt_file))\n",
    "print(chars)\n",
    "print('Total number of unique characters/Vocabulary size:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '('), (5, ')'), (6, '*'), (7, ','), (8, '-'), (9, '.'), (10, '0'), (11, '1'), (12, '2'), (13, '3'), (14, '4'), (15, '5'), (16, '6'), (17, '7'), (18, '8'), (19, '9'), (20, ':'), (21, ';'), (22, '?'), (23, 'A'), (24, 'B'), (25, 'C'), (26, 'D'), (27, 'E'), (28, 'F'), (29, 'G'), (30, 'H'), (31, 'I'), (32, 'J'), (33, 'K'), (34, 'L'), (35, 'M'), (36, 'N'), (37, 'O'), (38, 'P'), (39, 'Q'), (40, 'R'), (41, 'S'), (42, 'T'), (43, 'U'), (44, 'V'), (45, 'W'), (46, 'X'), (47, 'Y'), (48, '['), (49, ']'), (50, 'a'), (51, 'b'), (52, 'c'), (53, 'd'), (54, 'e'), (55, 'f'), (56, 'g'), (57, 'h'), (58, 'i'), (59, 'j'), (60, 'k'), (61, 'l'), (62, 'm'), (63, 'n'), (64, 'o'), (65, 'p'), (66, 'q'), (67, 'r'), (68, 's'), (69, 't'), (70, 'u'), (71, 'v'), (72, 'w'), (73, 'x'), (74, 'y'), (75, 'z'), (76, 'ç'), (77, 'é'), (78, 'ê'), (79, 'ô'), (80, '\\u200a'), (81, '—'), (82, '‘'), (83, '’'), (84, '“'), (85, '”'), (86, '…'), (87, '\\ufeff')]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder and decoder for characterwise tokenisation\n",
    "string_to_int = {\n",
    "    ch:i for i,ch in enumerate(chars)\n",
    "}\n",
    "\n",
    "int_to_string = {\n",
    "    i:ch for i,ch in enumerate(chars)\n",
    "}\n",
    "\n",
    "charwise_encoder = lambda input_word: [string_to_int[char] for char in input_word]\n",
    "charwise_decoder = lambda input_tokenised_word: ''.join([int_to_string[i] for i in input_tokenised_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 50, 56, 59, 58, 69, 1, 41, 58, 63, 56, 57]\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with tokeniser\n",
    "encoded_Jagjit_Singh = charwise_encoder('Jagjit Singh')\n",
    "print(encoded_Jagjit_Singh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jagjit Singh\n"
     ]
    }
   ],
   "source": [
    "decoded_Jagjit_Singh = charwise_decoder(encoded_Jagjit_Singh)\n",
    "print(decoded_Jagjit_Singh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching to tensors!\n",
    "\n",
    "Working with arrays is not a convinient option in context of machine learning. So, its better if one switches tensors which can be multi-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check for cuda\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([87, 31, 63,  1, 62, 74,  1, 74, 64, 70, 63, 56, 54, 67,  1, 50, 63, 53,\n",
      "         1, 62, 64, 67, 54,  1, 71, 70, 61, 63, 54, 67, 50, 51, 61, 54,  1, 74,\n",
      "        54, 50, 67, 68,  1, 62, 74,  1, 55, 50, 69, 57, 54, 67])\n"
     ]
    }
   ],
   "source": [
    "# Convert the txt_file from array/list to tensor\n",
    "data = torch.tensor(charwise_encoder(txt_file), dtype=torch.long)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_percentage_split = int(0.8*len(data))\n",
    "train_data = data[:train_percentage_split]\n",
    "val_data = data[train_percentage_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([87]) target: tensor(31)\n",
      "context: tensor([87, 31]) target: tensor(63)\n",
      "context: tensor([87, 31, 63]) target: tensor(1)\n",
      "context: tensor([87, 31, 63,  1]) target: tensor(62)\n",
      "context: tensor([87, 31, 63,  1, 62]) target: tensor(74)\n",
      "context: tensor([87, 31, 63,  1, 62, 74]) target: tensor(1)\n",
      "context: tensor([87, 31, 63,  1, 62, 74,  1]) target: tensor(74)\n",
      "context: tensor([87, 31, 63,  1, 62, 74,  1, 74]) target: tensor(64)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    targ = y[i]\n",
    "    print('context:', context, 'target:', targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "en23",
   "language": "python",
   "name": "kernel_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
