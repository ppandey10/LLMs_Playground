{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses GPT2 to create tiny Fitzgerald!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128' \n",
    "os.environ['https_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 00:50:12.516076: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-08 00:50:12.558024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 00:50:12.558083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 00:50:12.559081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 00:50:12.565874: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-08 00:50:12.567150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 00:50:15.671528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tokeniser\n",
    "gpt_tokeniser = GPT2Tokenizer.from_pretrained('openai-community/gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3666, 1438,  318, 1736,  292, 2049,    0]])\n"
     ]
    }
   ],
   "source": [
    "example_token = gpt_tokeniser.encode('My name is Prasoon!', return_tensors='pt')\n",
    "print(example_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice\n",
      "that I‚Äôve been turning over in my mind ever since.\n",
      "\n",
      "‚ÄúWhenever you feel like criticizing anyone,‚Äù he told me, ‚Äújust\n",
      "remember that all the people in this world haven‚Äôt had the advantages\n",
      "that you‚Äôve had.‚Äù\n",
      "\n",
      "He didn‚Äôt say any more, but we‚Äôve always been unusually communicative\n",
      "in a reserved way, and I understood that he meant a great deal more\n",
      "than that. In consequence, I‚Äôm inclined to reserve all judgements, a\n",
      "habit that has opened up many curious natures to me and also made me\n",
      "the victim of not a few veteran bores. \n"
     ]
    }
   ],
   "source": [
    "# Extract the # Load and read the text file\n",
    "with open('/Net/Groups/BGI/scratch/ppandey/LLMs_Playground/The_Great_Gatsby.txt', 'r', encoding='utf-8') as file:\n",
    "    txt_file = file.read() \n",
    "\n",
    "print(txt_file[:590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2', pad_token_id=gpt_tokeniser.eos_token_id)\n",
    "gpt_model = gpt_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   892, 16310,   265, 24754,  4528,   318,   257,  1049, 19553,\n",
      "           805]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Testing the model \n",
    "context_str = 'I think Virat Kohli is a great batsman'\n",
    "# Tokenise the string \n",
    "context_tkns = gpt_tokeniser.encode(context_str, return_tensors='pt').to('cuda')\n",
    "print(context_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the context tokens to my model\n",
    "output = gpt_model.generate(\n",
    "    inputs=context_tkns,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think Virat Kohli is a great batsman. I think he\\'s a very good player. He\\'s got a lot of experience in the game.\"\\n\\nKohli, who has been linked with a move to the West Indies, said: \"I don\\'t know if I\\'m going to be playing for England or not, but I\\'ve been playing cricket for a long time. It\\'s been a good experience for me.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokeniser.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated text\n",
    "text = gpt_tokeniser.decode(output[0], skip_special_tokens=True)\n",
    "with open('/Net/Groups/BGI/scratch/ppandey/LLMs_Playground/virat_kohli.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train my model on a custom dataset (The_Great_Gatsby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.data.datasets.language_modeling.TextDataset object at 0x7fbb7f4767a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the train dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=gpt_tokeniser,\n",
    "    file_path='/Net/Groups/BGI/scratch/ppandey/LLMs_Playground/The_Great_Gatsby.txt',\n",
    "    block_size=512,\n",
    ")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=gpt_tokeniser, \n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=200, # took about 600-700 epochs to get good results\n",
    "    per_device_train_batch_size=8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt_model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 09:25, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.05140870491663615, metrics={'train_runtime': 567.3751, 'train_samples_per_second': 53.228, 'train_steps_per_second': 1.058, 'total_flos': 7891019366400000.0, 'train_loss': 0.05140870491663615, 'epoch': 200.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_str = 'I enjoyed the counter-raid so thoroughly that I came back restless.'\n",
    "context_tkns = gpt_tokeniser.encode(context_str, return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gpt_model.generate(\n",
    "    inputs=context_tkns,\n",
    "    max_length=200,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = gpt_tokeniser.decode(output[0], skip_special_tokens=True)\n",
    "with open('/Net/Groups/BGI/scratch/ppandey/LLMs_Playground/GPT/fitzgerald_output.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
