{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SihsR00a82nG",
        "outputId": "e1386df8-730d-4e7d-b398-71722df5eb98"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['http_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128' \n",
        "os.environ['https_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (4.39.3)\n",
            "Requirement already satisfied: filelock in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (4.64.1)\n",
            "Requirement already satisfied: torch in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from transformers[torch]) (0.29.1)\n",
            "Requirement already satisfied: psutil in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from torch->transformers[torch]) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from torch->transformers[torch]) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->transformers[torch]) (59.5.0)\n",
            "Requirement already satisfied: wheel in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->transformers[torch]) (0.38.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zIBlARRamiGa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD32sTN9nKV6",
        "outputId": "b2e4f298-af81-494b-ff19-1e2f47f8fc3e"
      },
      "outputs": [],
      "source": [
        "# Extract the tokeniser\n",
        "tokeniser = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# Extract the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1XaJ-hJdnhem"
      },
      "outputs": [],
      "source": [
        "emotions = 'Pizza was awesome, but service was not good'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRiCwmRqoLRq",
        "outputId": "5899a4ee-a909-44e0-bcc1-1603d046a7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101, 59371, 10140, 37079, 42279, 10688,   117, 10502, 11416, 10140,\n",
            "         10497, 12050,   102]])\n"
          ]
        }
      ],
      "source": [
        "tokens = tokeniser.encode(emotions, return_tensors='pt') # pytorch\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRfkxeyEohyv",
        "outputId": "7e6f42c2-8d89-4bff-aef5-dd4624f8ba1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] pizza was awesome, but service was not good [SEP]\n"
          ]
        }
      ],
      "source": [
        "# Decode tokens back\n",
        "decoded_tokens = tokeniser.decode(tokens[0])\n",
        "print(decoded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAnRZos1o7Jo",
        "outputId": "e635a588-46da-4473-d663-5d94930f8f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9302,  1.1564,  0.8870, -0.5145, -1.9438]],\n",
            "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "tensor([[ 0.9302,  1.1564,  0.8870, -0.5145, -1.9438]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor(1)\n"
          ]
        }
      ],
      "source": [
        "# Feed the token to our pretrained model\n",
        "results = model(tokens)\n",
        "# Get results\n",
        "print(results)\n",
        "# Logits\n",
        "print(results.logits)\n",
        "# Max arg represents the sentiment predicted by the model\n",
        "print(torch.argmax(results.logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAurcD6hqGtT",
        "outputId": "b1a9d314-8f1a-44cd-ba8b-1ae7383e66b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-1.6949, -2.0449, -1.1944,  0.9835,  3.1879]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor(4)\n"
          ]
        }
      ],
      "source": [
        "# Experimenting with different emotions\n",
        "happy = 'Absolutely loved the idea!'\n",
        "happy_tkns = tokeniser.encode(happy, return_tensors='pt')\n",
        "happy_results = model(happy_tkns)\n",
        "print(happy_results.logits)\n",
        "print(torch.argmax(happy_results.logits))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuPnkpQiqM7e",
        "outputId": "3af70d76-62fa-46e6-e8f7-cc0e61810f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4.8434,  1.8983, -0.4402, -2.7727, -2.6937]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor(0)\n"
          ]
        }
      ],
      "source": [
        "sad = 'this bicycle is trash!'\n",
        "sad_tkns = tokeniser.encode(sad, return_tensors='pt')\n",
        "sad_results = model(sad_tkns)\n",
        "print(sad_results.logits)\n",
        "print(torch.argmax(sad_results.logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGMvZf4csFMV"
      },
      "source": [
        "So, basically lower values represent negative emotions and higher values represent positive emotions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAbjMOQZbPo5"
      },
      "source": [
        "# Fine-Tuning Models\n",
        "\n",
        "In this case, I worked with [DistilBERT base model (uncased)](https://huggingface.co/distilbert/distilbert-base-uncased)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crjLud_zdXYh",
        "outputId": "45ac8d1a-8dd4-4206-e978-eb55fd0a6015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5572, 2)\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "# Extract the dataset\n",
        "df = pd.read_csv('/Net/Groups/BGI/scratch/ppandey/LLMs_Playground/BERT/SMSSpamCollection',\n",
        "                 sep='\\t',\n",
        "                 names=[\"label\", \"message\"])\n",
        "print(df.shape)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1sREVm1fRC-",
        "outputId": "2878e124-a893-479c-a4a1-7e69c92d3df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...']\n"
          ]
        }
      ],
      "source": [
        "X = list(df[\"message\"])\n",
        "print(X[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uWNdZJwfrUq",
        "outputId": "091f1dd6-1ce9-4deb-cc74-25919baccbd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ham', 'ham']\n"
          ]
        }
      ],
      "source": [
        "Y = list(df[\"label\"])\n",
        "print(Y[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDn0hMOzfHYI",
        "outputId": "54c4b2a8-8ffc-424d-9e3d-91b702033e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Convert label from string to zeros and ones\n",
        "Y = list(pd.get_dummies(Y, drop_first=True)['spam'].astype(int))\n",
        "print(Y[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w_ioh8UsykfZ"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KZImumuN0idK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-07 23:10:56.797027: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-07 23:10:56.833687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-07 23:10:56.833744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-07 23:10:56.834707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-07 23:10:56.840968: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-04-07 23:10:56.841675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-07 23:11:02.729524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Import the model\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        ")\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJFYTIq1zj1t",
        "outputId": "9a6244a2-ccc4-483e-be1f-83cb6ff2bd94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Call the distilbert tokenizer\n",
        "db_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Call the model\n",
        "db_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "db_model = db_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEcDbCXy0v_O",
        "outputId": "60d51c9b-1b80-43ef-94e7-5742a3dca5a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101,  2026,  2171,  2003, 10975,  3022,  7828,   999,   102]])\n"
          ]
        }
      ],
      "source": [
        "example_token = db_tokenizer.encode('My name is Prasoon!', return_tensors='pt')\n",
        "print(example_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fadVW2_C2ItX"
      },
      "outputs": [],
      "source": [
        "# Encode the training and testing datasett\n",
        "encoded_train = db_tokenizer(x_train, truncation=True, padding=True)\n",
        "encoded_test = db_tokenizer(x_test, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlV2DyJU38TP",
        "outputId": "964246d3-c6bc-450c-9c6f-8005d650a926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Encoding(num_tokens=238, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=238, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
            "[Encoding(num_tokens=140, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=140, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_train[:2])\n",
        "print(encoded_test[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIWD1Fbi5BKT",
        "outputId": "fb21f18c-1b63-4757-ebfe-60b45d5049b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "[[101, 7514, 2000, 2663, 27708, 4882, 999, 2073, 2097, 1996, 2294, 5713, 2088, 2452, 2022, 2218, 1029, 4604, 2644, 2000, 6584, 21926, 2683, 2000, 2203, 2326, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7592, 1012, 4066, 1997, 2041, 1999, 2237, 2525, 1012, 2008, 1012, 2061, 2123, 2102, 5481, 2188, 1010, 1045, 2572, 5983, 6583, 9905, 2015, 1012, 2097, 2292, 2017, 2113, 27859, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_train.keys())\n",
        "print(encoded_train['input_ids'][:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "293l6MLW5Z-E"
      },
      "outputs": [],
      "source": [
        "# Create torch dataset (https://huggingface.co/transformers/v3.2.0/custom_datasets.html)\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu91TwUy8ArE",
        "outputId": "cb724f3f-6f34-448b-ed17-467ff52b8f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  7514,  2000,  2663, 27708,  4882,   999,  2073,  2097,  1996,\n",
            "          2294,  5713,  2088,  2452,  2022,  2218,  1029,  4604,  2644,  2000,\n",
            "          6584, 21926,  2683,  2000,  2203,  2326,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  7592,  1012,  4066,  1997,  2041,  1999,  2237,  2525,  1012,\n",
            "          2008,  1012,  2061,  2123,  2102,  5481,  2188,  1010,  1045,  2572,\n",
            "          5983,  6583,  9905,  2015,  1012,  2097,  2292,  2017,  2113, 27859,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0])}\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(encoded_train, y_train)\n",
        "test_dataset = Dataset(encoded_test, y_test)\n",
        "print(train_dataset[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zVxebvl561IC"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    print(type(p))\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL0GhdDs8S62",
        "outputId": "081576ac-457d-4fd6-c0c2-59b2d20a729c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "# Define Trainer\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8\n",
        "\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=db_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "y9ZxPXfb8wYl",
        "outputId": "d939886d-56f3-47c1-86b0-6a35d49ba8c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [70/70 00:13, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=70, training_loss=0.11615197317940848, metrics={'train_runtime': 28.1149, 'train_samples_per_second': 158.528, 'train_steps_per_second': 2.49, 'total_flos': 274447094927208.0, 'train_loss': 0.11615197317940848, 'epoch': 1.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bfhknKFnAtRp",
        "outputId": "acbdfb32-9ff0-4d47-909f-f86a993bc57f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.trainer_utils.EvalPrediction'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.03388119488954544,\n",
              " 'eval_accuracy': 0.9928251121076234,\n",
              " 'eval_precision': 0.9795918367346939,\n",
              " 'eval_recall': 0.9664429530201343,\n",
              " 'eval_f1': 0.9729729729729729,\n",
              " 'eval_runtime': 14.3647,\n",
              " 'eval_samples_per_second': 77.621,\n",
              " 'eval_steps_per_second': 1.253,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "yv9GH_r8ArlK",
        "outputId": "975a9daa-8ac5-4742-ea24-d0fa0a081216"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/Net/Groups/BGI/scratch/ppandey/miniconda3/envs/llm_venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.trainer_utils.EvalPrediction'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[ 2.987157 , -2.484671 ],\n",
              "       [ 3.0913355, -2.485971 ],\n",
              "       [ 3.0936885, -2.5021067],\n",
              "       ...,\n",
              "       [ 3.1390152, -2.5200768],\n",
              "       [ 3.0432382, -2.4372091],\n",
              "       [ 2.9266534, -2.339787 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.03388119488954544, 'test_accuracy': 0.9928251121076234, 'test_precision': 0.9795918367346939, 'test_recall': 0.9664429530201343, 'test_f1': 0.9729729729729729, 'test_runtime': 14.3641, 'test_samples_per_second': 77.624, 'test_steps_per_second': 1.253})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing the model\n",
        "trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP0WKLhUB0N1",
        "outputId": "3353350e-4e06-45c6-9001-3aac69790de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,   102]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "test_spam_text = \"WINNER!! As a valued network customer\"\n",
        "spam_tkns = db_tokenizer(test_spam_text, truncation=True, padding=True, return_tensors='pt').to('cuda')\n",
        "print(spam_tkns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRQ3c-JJDhcN",
        "outputId": "9054dc72-4cb9-4fc8-9d50-bf25449de686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2705, -0.1136]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "results = db_model(**spam_tkns)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChAzgkWMEz2i",
        "outputId": "1d02b5d8-63eb-4ccc-e0cd-1d2dc9d8c93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.5949, 0.4051]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "[[0.59486663 0.40513337]]\n"
          ]
        }
      ],
      "source": [
        "predictions = torch.nn.functional.softmax(results.logits, dim=-1)\n",
        "print(predictions)\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_env",
      "language": "python",
      "name": "llm_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
